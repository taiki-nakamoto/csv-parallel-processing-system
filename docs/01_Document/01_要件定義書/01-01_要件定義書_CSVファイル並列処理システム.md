# CSVファイル並列処理システム機能要件定義書

## 1. システム概要

### 1.1 目的
ユーザーのログイン回数や投稿回数を含むCSVファイル（1000行レベル）に対して、AWS Step Functions分散マップ機能を活用した並列処理でAurora DBに反映する信頼性の高いユーザー統計更新システムを構築する。

### 1.2 システムの特徴
- **適度な並列性**: 最大5並列実行による効率的データ処理
- **イベント駆動**: S3ファイルアップロード時の自動実行
- **重複実行防止**: ファイル名ベースの重複処理制御
- **堅牢性**: 包括的なエラーハンドリングと自動復旧
- **監視・通知**: 処理状況監視とエラー通知

### 1.3 対象範囲
- ユーザーログCSVファイルの自動検知と処理起動
- ユーザー統計情報の並列処理とAurora DB更新
- エラー管理とデータ整合性保証
- 処理状況監視と運用ログ

## 2. 機能要件

### 2.1 ファイル入力・検知機能

#### 2.1.1 S3イベント検知機能
**機能ID**: F001  
**機能名**: S3ファイルアップロード自動検知  
**概要**: S3バケットにCSVファイルがアップロードされた際の自動検知

**詳細仕様**:
- **対象イベント**: ObjectCreated:Put, ObjectCreated:Post
- **対象ファイル**: 拡張子が`.csv`のファイル
- **除外ファイル**: ファイル名が`_`で始まるファイル（一時ファイル）
- **検知遅延**: イベント発生から1秒以内
- **通知方式**: EventBridgeによるイベント配信

**入力条件**:
- ファイル名が一意であること
- CSVファイルが正常な形式であること
- ファイルサイズが100MB以下であること
- 行数が10,000行以下であること
- CSVフォーマット: `ユーザーID,ログイン回数,投稿回数` 形式

#### 2.1.2 Step Functions自動起動機能
**機能ID**: F002  
**機能名**: EventBridgeからStep Functions直接呼び出し  
**概要**: S3イベントを受けてEventBridgeが直接Step Functionsを起動

**詳細仕様**:
- **トリガー**: S3 ObjectCreatedイベント
- **呼び出し方式**: EventBridgeルールから直接Step Functions実行
- **パラメータ渡し**: S3バケット名、オブジェクトキーを渡し
- **実行名生成**: S3ファイル名から一意な実行名を生成
- **レスポンス時間**: イベント発生から3秒以内

#### 2.1.3 ファイル検証機能
**機能ID**: F003  
**機能名**: CSVファイル形式検証  
**概要**: Step Functionsの最初のステートで実行されるCSVファイル検証

**詳細仕様**:
- **ヘッダー検証**: CSVヘッダーの存在と形式確認
  - 期待ヘッダー: `ユーザーID,ログイン回数,投稿回数`
- **文字コード確認**: UTF-8エンコーディングの確認
- **区切り文字確認**: カンマ区切りの確認
- **データ形式検証**: ユーザーID形式、数値フィールドの検証
- **基本統計**: 行数、列数、ファイルサイズの取得
- **検証時間**: 5分以内での完了

**出力**:
- 検証結果（成功/失敗）
- ファイル統計情報
- エラー詳細（失敗時）

### 2.2 並列処理機能

#### 2.2.1 分散マップ処理機能
**機能ID**: F004  
**機能名**: CSV行単位並列処理  
**概要**: Step Functions分散マップによるCSV行の並列処理

**詳細仕様**:
- **処理単位**: CSV1行 = 1アイテム
- **並列度**: 最大5並列実行（設定可能）
- **チャンク処理**: 中規模行の効率的分割処理
- **実行モード**: DISTRIBUTED + STANDARD実行
- **タイムアウト**: アイテム処理最大15分

**処理パターン**:
1. **単行処理**: 1行 = 1Lambda実行
2. **チャンク処理**: 複数行 = 1Lambda実行（効率化）

#### 2.2.2 Lambda行処理機能
**機能ID**: F005  
**機能名**: ユーザーログデータ処理・Aurora DB更新  
**概要**: ユーザーのログイン回数や投稿回数をAurora DBに反映する処理

**入力データ例**:
```json
{
  "ユーザーID": "U00001",
  "ログイン回数": "2",
  "投稿回数": "10"
}
```

**詳細仕様**:
- **入力**: JSONオブジェクト化されたユーザーログデータ
- **処理内容**: 
  - ユーザーIDの存在確認（Aurora DB照会）
  - ログイン回数、投稿回数の数値検証
  - ユーザー統計テーブルの更新（UPSERT処理）
  - 処理履歴ログの記録
- **Aurora DB操作**: 
  - ユーザー情報参照: `usersテーブル`
  - 統計情報更新: `user_statisticsテーブル`
- **出力**: 更新結果情報またはエラー情報
- **実行時間**: 1行あたり最大30秒

### 2.3 重複実行防止機能

#### 2.3.1 実行名ベース制御機能
**機能ID**: F006  
**機能名**: ファイル名ベース重複実行防止  
**概要**: S3ファイル名をキーとした重複実行の自動防止

**詳細仕様**:
- **実行名生成**: S3ファイル名から80文字以内の一意実行名
- **重複チェック**: Step Functions実行名の一意性制約活用
- **制御方式**: `ExecutionAlreadyExistsException`による自動制御
- **対象**: 同一ステートマシン内での重複実行

**実行名生成ルール**:
```
S3キー: "data/report-2024-01-15.csv"
実行名: "report-2024-01-15"
```

#### 2.3.2 EventBridge重複対策機能
**機能ID**: F007  
**機能名**: イベント重複配信対策  
**概要**: EventBridgeイベント重複配信に対する自動保護

**詳細仕様**:
- **検知方式**: 実行名重複による自動検知
- **対応**: 2回目以降の実行要求を自動拒否
- **ログ記録**: 重複試行の詳細ログ保存
- **メトリクス**: 重複発生状況のCloudWatch記録

### 2.4 エラー処理・復旧機能

#### 2.4.1 階層的エラーハンドリング機能
**機能ID**: F008  
**機能名**: 多層エラー処理システム  
**概要**: Lambda、Step Functions、分散マップの各レベルでの包括的エラー処理

**詳細仕様**:
- **Lambdaレベル**: 業務エラーとシステムエラーの分類・処理
- **ステートレベル**: Retry/Catch設定による自動回復
- **分散マップレベル**: ToleratedFailurePercentage/Count設定
- **失敗許容率**: 全体の5%または最大50件まで許容（設定可能）

**エラー分類**:
- **一時的エラー**: 自動リトライ対象
- **永続的エラー**: DLQ送信対象
- **業務エラー**: カスタムハンドリング対象

#### 2.4.2 DynamoDBログ記録機能
**機能ID**: F009  
**機能名**: 全処理ログ永続化管理  
**概要**: 処理結果（正常・エラー両方）の監査ログをDynamoDBに記録・管理

**詳細仕様**:
- **記録対象**: 全ユーザーデータ処理結果（成功・失敗両方）
- **記録項目**: 
  - 実行名、タイムスタンプ、ユーザーID
  - 処理タイプ（login_update/post_update）
  - 更新前後の値（old_value/new_value）
  - 処理状態（success/failed/skipped）
  - エラー情報（失敗時のみ）
- **保存期間**: 6ヶ月間（TTL自動削除）
- **検索機能**: 実行名・ユーザーID・タイムスタンプでの検索
- **コスト効率**: オンデマンド課金で月額$2-3程度

### 2.5 結果出力・集約機能

#### 2.5.1 処理結果集約機能
**機能ID**: F010  
**機能名**: 分散処理結果の統合・出力  
**概要**: 並列処理結果のS3出力と統計情報生成

**詳細仕様**:
- **出力形式**: JSON、CSV（設定可能）
- **出力先**: 指定S3バケット
- **統計情報**: 
  - 総処理ユーザー数、成功更新数、失敗件数
  - Aurora DB更新結果サマリー
  - 処理時間、スループット
  - ユーザー統計更新エラー統計
- **ファイル出力**: 処理結果サマリーファイルの出力

## 3. 非機能要件

### 3.1 性能要件

#### 3.1.1 処理性能
- **スループット**: 1,000行/5分以内
- **並列度**: 最大5並列実行
- **レスポンス**: イベント検知から処理開始まで5秒以内
- **処理完了**: 1,000行CSVを5分以内で完了

#### 3.1.2 可用性
- **システム可用性**: 99.9%以上
- **障害復旧時間**: 15分以内
- **データ整合性**: 100%保証

### 3.2 拡張性要件

#### 3.2.1 データ規模拡張
- **ファイルサイズ**: 最大100MB対応
- **行数**: 最大10,000行対応
- **同時処理**: 複数ファイルの同時処理対応（最大3ファイル並行）

#### 3.2.2 機能拡張
- **処理ロジック追加**: Lambda関数の独立更新
- **新ファイル形式**: TSV、JSON等への対応拡張
- **新出力形式**: 各種データ形式への出力対応

### 3.3 運用要件

#### 3.3.1 監視要件
- **リアルタイム監視**: CloudWatch Dashboardによる状況把握
- **アラート**: 異常検知時の自動通知
- **ログ保存**: 30日間の詳細ログ保存

#### 3.3.2 保守要件
- **設定変更**: 並列度、タイムアウト等の動的変更
- **バージョン管理**: Lambda関数、設定の版数管理
- **テスト環境**: 本番同等のテスト環境提供

## 4. システム制約事項

### 4.1 AWS制約
- **Step Functions**: 分散マップ最大5並列実行（設定制限）
- **Lambda**: 実行時間最大5分、メモリ最大1GB
- **実行名**: 80文字以内、英数字・ハイフン・アンダースコアのみ

### 4.2 データ制約
- **ファイル形式**: CSV（UTF-8エンコーディング）
- **ファイル名**: 一意性必須
- **区切り文字**: カンマ区切り

### 4.3 運用制約
- **処理時間帯**: 24時間365日対応
- **メンテナンス**: 月次定期メンテナンス
- **データ保存**: エラーログ30日、処理ログ7日

## 5. インターフェース仕様

### 5.1 入力インターフェース
- **S3バケット**: CSV形式ファイルのアップロード
- **EventBridge**: S3イベント通知の受信
- **設定パラメータ**: 環境変数・Parameter Storeによる設定

### 5.2 出力インターフェース
- **S3出力**: 処理結果ファイルの出力
- **DynamoDB**: エラー・統計情報の記録
- **CloudWatch**: メトリクス・ログの出力
- **通知**: SNS/Slack/Email通知

## 6. セキュリティ要件

### 6.1 アクセス制御
- **IAM**: 最小権限の原則に基づくロール設計
- **S3**: バケットポリシーによるアクセス制限
- **暗号化**: 保存時・転送時の暗号化

### 6.2 データ保護
- **機密データ**: 個人情報等の適切なマスキング
- **ログ**: 機密情報のログ出力制限
- **監査**: アクセス・操作ログの記録

## 7. 運用・保守

### 7.1 監視項目
- **実行状況**: 成功/失敗件数、実行時間
- **エラー率**: 時間別・日別エラー発生率
- **リソース使用量**: Lambda実行時間、DynamoDB使用量

### 7.2 障害対応
- **エスカレーション**: 重要度別の対応体制
- **復旧手順**: 標準的な障害復旧プロセス
- **データ修復**: エラーレコードの再処理機能

## 8. 今後の拡張予定

### 8.1 短期拡張（3ヶ月以内）
- **リアルタイム処理**: Kinesis連携によるストリーミング処理
- **WebUI**: 処理状況確認用の管理画面

### 8.2 中期拡張（6ヶ月以内）
- **ML連携**: Amazon SageMaker連携による機械学習処理
- **マルチリージョン**: 災害対策用のマルチリージョン展開

### 8.3 長期拡張（1年以内）
- **API化**: REST API提供による外部システム連携
- **スケジュール処理**: 定期実行機能の追加